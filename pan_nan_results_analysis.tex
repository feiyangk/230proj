\section{Results Analysis}

\subsection{Input Features}

The PAN-NAN fusion model processes a total of 118 input features, organized into several categories:

\textbf{Price Features (27 features):}
The model uses closing prices from 27 tickers: SPY, QQQ, IWM, RSP, XLF, XLI, XLY, XLK, XLE, XLB, XLP, XLV, XLU, XLRE, TLT, IEF, TIP, SHY, LQD, HYG, DBC, USO, GLD, DBA, WEAT, SOYB, UUP. These are processed by the PAN branch, either directly (without FinCast) or through FinCast embeddings (with FinCast enabled).

\textbf{Volume Features (27 features):}
Trading volume for each of the 27 tickers, processed as synthetic features in the PAN branch.

\textbf{Technical Indicators (54 features):}
Simple moving averages for each ticker:
\begin{itemize}
    \item SMA-50: 27 features (sma\_50\_SPY, sma\_50\_QQQ, \ldots, sma\_50\_UUP)
    \item SMA-200: 27 features (sma\_200\_SPY, sma\_200\_QQQ, \ldots, sma\_200\_UUP)
\end{itemize}
These technical indicators are processed alongside price features in the PAN branch.

\textbf{Time Features (3 features):}
Cyclical and categorical time encodings:
\begin{itemize}
    \item month\_sin: Sinusoidal encoding of month (captures cyclical seasonality)
    \item month\_cos: Cosine encoding of month (complements month\_sin)
    \item is\_weekend: Binary indicator for weekend days
\end{itemize}
These time-varying known features are included in the PAN branch input.

\textbf{GDELT Sentiment Features (7 features):}
Sentiment indicators from the Global Database of Events, Language, and Tone (GDELT), processed by the NAN branch:
\begin{itemize}
    \item weighted\_avg\_tone: Average sentiment tone of news articles
    \item weighted\_avg\_polarity: Average polarity of sentiment
    \item num\_articles: Number of articles mentioning inflation-related topics
    \item num\_sources: Number of unique news sources
    \item sentiment\_lag\_1: 1-day lagged sentiment (captures recent sentiment momentum)
    \item sentiment\_lag\_7: 7-day lagged sentiment (captures weekly sentiment trends)
    \item sentiment\_lag\_30: 30-day lagged sentiment (captures monthly sentiment patterns)
\end{itemize}

\textbf{Feature Distribution:}
\begin{itemize}
    \item \textbf{PAN Branch Input:} 27 (price) + 27 (volume) + 27 (SMA-50) + 27 (SMA-200) + 3 (time) = 111 features
    \item \textbf{NAN Branch Input:} 7 (GDELT sentiment) features
    \item \textbf{Total Input Features:} 118 features
\end{itemize}

The PAN branch processes price, volume, technical indicators, and time features through a decoder transformer with Multi-Head Masked Self-Attention (MMSA), while the NAN branch processes GDELT sentiment features through a Conv1D + Transformer encoder + BiGRU pipeline. The fusion mechanism combines predictions from both branches to generate multi-horizon forecasts.

\subsection{Overall Forecasting Performance}

The PAN-NAN fusion model demonstrates strong predictive performance on the validation set, achieving a MAE of 0.7392 (without FinCast) and 0.7496 (with FinCast), with directional accuracy of 68.63\% for both variants. Compared to the LSTM baseline (MAE = 0.5850, directional accuracy = 51.00\%), PAN-NAN achieves a 26.4\% improvement in directional accuracy, indicating that the dual-branch architecture effectively captures multi-source signals---price dynamics via the PAN branch and sentiment-driven inflation expectations via the NAN branch.

The high directional accuracy (68.63\%) compared to LSTM (51.00\%) shows that the model consistently identifies the correct direction of movement in the agricultural-inflation proxy. This is particularly important because inflation shocks propagate through energy prices, food commodities, and sentiment channels before appearing in macro statistics. The PAN branch's decoder transformer with Multi-Head Masked Self-Attention (MMSA) captures price momentum and technical patterns, while the NAN branch's transformer encoder processes GDELT sentiment features to extract inflation expectations. The fusion mechanism learns optimal weighting between these complementary information sources.

On the test set, PAN-NAN-No-Fincast achieves a test MAE of 0.510364 and RMSE of 0.656283, with directional accuracy of 66.78\%, outperforming TFT-Final (test MAE = 0.845457, directional accuracy = 29.89\%) and TFT-No-Fincast (test MAE = 0.617535, directional accuracy = 47.13\%). This demonstrates that the dual-branch fusion architecture generalizes well to unseen data and maintains effectiveness on the test set.

\subsection{Benchmark Comparison}

As shown in Table~\ref{tab:benchmark_comparison}, PAN-NAN fusion models are compared against LSTM, TFT-No-Fincast, and TFT-Final baselines across validation and test metrics.

\textbf{Validation Set Performance:}
\begin{itemize}
    \item \textbf{PAN-NAN-No-Fincast} achieves validation MAE of 0.7392, outperforming LSTM (0.5850) but with higher MAE than TFT-Final (0.533402). However, directional accuracy of 68.63\% significantly exceeds LSTM (51.00\%) and is comparable to TFT variants (70.50\% for TFT-Final, 71.65\% for TFT-No-Fincast).
    \item \textbf{PAN-NAN-With-Fincast} shows similar validation MAE (0.7496) and identical directional accuracy (68.63\%) compared to the No-Fincast variant, suggesting that FinCast backbone does not substantially improve validation performance in this setting.
    \item Both PAN-NAN variants demonstrate strong directional accuracy (68.63\%) compared to LSTM (51.00\%), indicating that the dual-branch architecture effectively captures directional patterns.
\end{itemize}

\textbf{Test Set Performance:}
\begin{itemize}
    \item \textbf{PAN-NAN-No-Fincast} achieves the best test MAE (0.510364) among all models, outperforming TFT-Final (0.845457), TFT-No-Fincast (0.617535), and LSTM (0.459052). Test RMSE of 0.656283 is also superior to TFT variants (0.804553 and 1.038776).
    \item \textbf{Directional accuracy on test set:} PAN-NAN-No-Fincast achieves 66.78\%, significantly outperforming TFT-Final (29.89\%) and TFT-No-Fincast (47.13\%), though lower than LSTM (47.98\%). This suggests that PAN-NAN fusion maintains strong directional prediction capability on unseen data.
    \item \textbf{PAN-NAN-With-Fincast} shows higher test MAE (0.535234) and lower directional accuracy (33.55\%) compared to the No-Fincast variant, indicating that FinCast may not improve generalization in this setting.
\end{itemize}

\textbf{Interpretation:}
The PAN-NAN fusion architecture demonstrates superior test set performance compared to TFT variants, particularly in directional accuracy. The dual-branch design---combining price attention (PAN) and sentiment processing (NAN)---captures complementary signals that generalize better to unseen data than single-branch TFT models. However, the FinCast backbone does not consistently improve performance, suggesting that raw price features may be sufficient for this task.

\subsection{Test Set Performance and Robustness}

PAN-NAN-No-Fincast demonstrates strong generalization from validation to test set. While validation MAE is 0.7392, test MAE improves to 0.510364, representing a 31.0\% reduction in error. This improvement suggests that the model learns robust patterns that generalize well to unseen data. In contrast, TFT-Final shows significant degradation from validation (MAE = 0.533402) to test (MAE = 0.845457), representing a 58.5\% increase in error.

The directional accuracy remains relatively stable: PAN-NAN-No-Fincast achieves 68.63\% on validation and 66.78\% on test (only 1.85 percentage point decrease), compared to TFT-Final which drops from 70.50\% to 29.89\% (40.61 percentage point decrease). This indicates that PAN-NAN fusion maintains its directional prediction capability better than TFT variants when generalizing to test data.

\textbf{Robustness Analysis:}
\begin{itemize}
    \item PAN-NAN-No-Fincast shows consistent performance across validation and test sets, with test MAE (0.510364) actually lower than validation MAE (0.7392), suggesting good generalization.
    \item The model maintains high directional accuracy (66.78\%) on test set, indicating robustness of the fusion mechanism.
    \item FinCast variant shows worse generalization: test MAE (0.535234) is lower than validation MAE (0.7496), but directional accuracy drops significantly from 68.63\% to 33.55\%.
\end{itemize}

\subsection{Horizon-Specific Performance}

Per-horizon analysis reveals distinct patterns across forecasting horizons (see Table~\ref{tab:benchmark_comparison}):

\textbf{Short-Term Horizon (H7):}
\begin{itemize}
    \item PAN-NAN-No-Fincast achieves H7 MAE of 0.371871, outperforming TFT-Final (0.488511) and TFT-No-Fincast (0.402471), and comparable to LSTM (0.372671).
    \item This suggests that price signals (PAN branch) dominate short-term predictions, and the fusion mechanism effectively combines price and sentiment for near-term forecasts.
\end{itemize}

\textbf{Medium-Term Horizon (H14):}
\begin{itemize}
    \item PAN-NAN-No-Fincast achieves H14 MAE of 0.516173, outperforming TFT-Final (0.826217) and TFT-No-Fincast (0.600955), but higher than LSTM (0.456582).
    \item The balanced fusion of price and sentiment signals becomes more important at medium horizons, where both PAN and NAN branches contribute meaningfully.
\end{itemize}

\textbf{Long-Term Horizon (H28):}
\begin{itemize}
    \item PAN-NAN-No-Fincast achieves H28 MAE of 0.643049, significantly outperforming TFT-Final (1.221644) and TFT-No-Fincast (0.849179), and outperforming LSTM (0.547903).
    \item Sentiment signals (NAN branch) may become more important for longer horizons, as the model leverages GDELT features to capture inflation expectations that manifest over longer time scales.
\end{itemize}

\textbf{Error Growth Pattern:}
The error growth from H7 to H28 follows expected patterns: PAN-NAN-No-Fincast shows error growth of 73.0\% (from 0.371871 to 0.643049), which is more moderate than TFT-Final (150.2\% growth) and TFT-No-Fincast (111.0\% growth). This suggests that the dual-branch architecture maintains predictive power better across horizons compared to single-branch TFT models.

\textbf{FinCast Impact by Horizon:}
PAN-NAN-With-Fincast shows higher MAE across all horizons compared to No-Fincast variant (H7: 0.513744 vs. 0.371871, H14: 0.713916 vs. 0.516173, H28: 0.988153 vs. 0.643049), suggesting that FinCast backbone does not improve performance at any horizon in this setting.

\subsection{Key Takeaways}

\textbf{Architectural Insights:}

\textbf{1. Dual-Branch Design Effectiveness:}
The PAN-NAN fusion architecture successfully integrates complementary information sources. The PAN branch, with its decoder transformer and MMSA mechanisms, captures price momentum, technical indicators, and cross-asset relationships. The NAN branch, with its Conv1D + Transformer encoder + BiGRU pipeline, processes GDELT sentiment features to extract inflation expectations. The fusion mechanism learns optimal weighting between these signals, resulting in superior test set performance (MAE = 0.510364) compared to TFT variants.

\textbf{2. FinCast Backbone Analysis:}
The FinCast backbone does not consistently improve performance in this setting. PAN-NAN-With-Fincast shows similar validation MAE (0.7496 vs. 0.7392) but worse test performance (MAE = 0.535234 vs. 0.510364, directional accuracy = 33.55\% vs. 66.78\%). This suggests that raw price features may be sufficient for this task, and the pre-trained FinCast embeddings may not add value or may even introduce noise. The trade-off between pre-trained features and model complexity does not favor FinCast in this case.

\textbf{3. Sentiment Integration (NAN Branch):}
The NAN branch's contribution is evident in the model's strong performance, particularly at longer horizons. While PAN-NAN-No-Fincast achieves comparable H7 MAE to LSTM (0.371871 vs. 0.372671), it significantly outperforms TFT variants at H14 and H28, suggesting that sentiment features become more important for medium and long-term forecasts. The NAN branch's transformer encoder effectively processes temporal sentiment patterns, and the Conv1D + Transformer + BiGRU pipeline captures sentiment dynamics that complement price signals.

\textbf{4. Fusion Mechanism Behavior:}
The fusion mechanism effectively combines PAN and NAN predictions, as evidenced by PAN-NAN-No-Fincast outperforming both TFT variants (which can be viewed as single-branch models) on the test set. The fusion layer's MLP learns non-linear combinations that adapt to different market regimes, maintaining high directional accuracy (66.78\%) on test data while TFT-Final drops to 29.89\%.

\textbf{5. Horizon-Specific Patterns:}
Error growth patterns reveal horizon-specific strengths: short-term (H7) predictions benefit from price signals (PAN branch), while longer horizons (H14, H28) leverage both price and sentiment. The moderate error growth (73.0\% from H7 to H28) compared to TFT variants (150.2\% for TFT-Final) indicates that the dual-branch architecture maintains predictive power better across horizons.

\textbf{6. Comparison with Baselines:}
PAN-NAN fusion demonstrates superior test set performance compared to TFT variants, achieving the best test MAE (0.510364) and maintaining high directional accuracy (66.78\%). The dual-branch design captures multi-modal signals more effectively than single-branch TFT models, as evidenced by the significant performance gap on test data. Compared to LSTM, PAN-NAN achieves much higher directional accuracy (66.78\% vs. 47.98\%) while maintaining competitive MAE, indicating that attention mechanisms provide better temporal modeling than recurrent architectures.

\textbf{7. Limitations and Future Directions:}
While PAN-NAN fusion shows strong performance, several limitations warrant future investigation: (1) FinCast backbone does not improve performance, suggesting alternative backbone options or feature engineering may be more effective; (2) computational complexity of dual-branch architecture requires evaluation of efficiency trade-offs; (3) fusion mechanism interpretability could be enhanced to understand learned weightings between PAN and NAN branches; (4) sentiment feature quality and coverage from GDELT may limit NAN branch effectiveness in certain regimes.

\textbf{Qualitative Conclusions:}
PAN-NAN fusion successfully integrates price and sentiment signals through its dual-branch architecture, achieving superior test set performance (MAE = 0.510364, directional accuracy = 66.78\%) compared to TFT variants. The model demonstrates strong generalization from validation to test set, maintaining high directional accuracy while improving magnitude accuracy. The FinCast backbone does not improve performance in this setting, suggesting that raw price features may be sufficient. Sentiment integration via the NAN branch adds value, particularly for longer horizons, and the fusion mechanism effectively combines multi-modal signals while maintaining interpretability through branch separation.

\subsection{Attention Visualization Analysis}

Attention visualization from the PAN branch (see Section~\ref{sec:attention}) reveals what the model attends to in price and synthetic features. The Multi-Head Masked Self-Attention (MMSA) layers in the PAN branch's decoder transformer show temporal attention patterns across the 192-timestep lookback window. Analysis of attention heatmaps indicates that the model focuses on recent price movements and technical indicator patterns, with attention weights distributed across multiple heads to capture different aspects of price dynamics. The causal masking ensures that the model only attends to past timesteps, maintaining temporal causality in predictions.


